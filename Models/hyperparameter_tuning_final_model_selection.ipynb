{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f076fc4c",
   "metadata": {},
   "source": [
    "# 🚀 Advanced Hyperparameter Tuning for Top 3 Performing Models (Speed Optimized)\n",
    "\n",
    "This notebook performs comprehensive hyperparameter optimization and class imbalance handling for the top 3 models identified from baseline analysis. Each model will be optimized separately with advanced techniques optimized for fast execution.\n",
    "\n",
    "## Top 3 Models to Optimize:\n",
    "1. **Gradient Boosting** (AUC-ROC: 0.8390)\n",
    "2. **CatBoost** (AUC-ROC: 0.8356) \n",
    "3. **AdaBoost** (AUC-ROC: 0.8345)\n",
    "\n",
    "## Advanced Techniques Applied:\n",
    "- **Class Imbalance**: SMOTE-Tomek (focused hybrid sampling)\n",
    "- **Optimization**: Optuna Bayesian Search (20 trials per model for speed)\n",
    "- **Validation**: 5-fold Stratified Cross-Validation\n",
    "- **Ensemble**: Voting & Stacking Classifiers\n",
    "- **Goal**: Maximize performance with ultra-fast execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9966613",
   "metadata": {},
   "source": [
    "## 📂 Load Data\n",
    "Load the feature-engineered training dataset and prepare features (X) and target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2fc61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading feature-engineered training dataset...\n",
      "Dataset shape: (5625, 22)\n",
      "Target variable encoded (No=0, Yes=1)\n",
      "\n",
      "📊 Class Distribution:\n",
      "Class 0 (No Churn): 4130 (73.42%)\n",
      "Class 1 (Churn): 1495 (26.58%)\n",
      "Imbalance Ratio: 2.76:1\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "✅ Data prepared successfully!\n",
      "Features shape: (5625, 20), Target shape: (5625,)\n",
      "Training set: (4500, 20), Validation set: (1125, 20)\n"
     ]
    }
   ],
   "source": [
    "# Import comprehensive libraries for advanced techniques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Class Imbalance Handling\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "from collections import Counter\n",
    "\n",
    "# Advanced Models\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "print(\"📂 Loading feature-engineered training dataset...\")\n",
    "data = pd.read_csv('../Data/output/feature_engineered_train.csv')\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['customerID', 'Churn'])\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode target if needed\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    print(\"Target variable encoded (No=0, Yes=1)\")\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = Counter(y)\n",
    "print(f\"\\n📊 Class Distribution:\")\n",
    "print(f\"Class 0 (No Churn): {class_counts[0]} ({class_counts[0]/len(y)*100:.2f}%)\")\n",
    "print(f\"Class 1 (Churn): {class_counts[1]} ({class_counts[1]/len(y)*100:.2f}%)\")\n",
    "print(f\"Imbalance Ratio: {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = data.isnull().sum().sum()\n",
    "print(f'\\nMissing values: {missing}')\n",
    "assert missing == 0, 'There are missing values in the data!'\n",
    "\n",
    "print(f'\\n✅ Data prepared successfully!')\n",
    "print(f'Features shape: {X.shape}, Target shape: {y.shape}')\n",
    "\n",
    "# Create train-validation split for proper evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n",
    "                                                  stratify=y, random_state=42)\n",
    "print(f'Training set: {X_train.shape}, Validation set: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f295399",
   "metadata": {},
   "source": [
    "## 🎯 Top 3 Models for Advanced Optimization (Speed Optimized)\n",
    "\n",
    "Based on baseline model evaluation, we will focus on the top 3 performing models for fast execution:\n",
    "\n",
    "1. **Gradient Boosting Classifier** (AUC-ROC: 0.8390) - Best overall performer\n",
    "2. **CatBoost Classifier** (AUC-ROC: 0.8356) - Strong gradient boosting variant  \n",
    "3. **AdaBoost Classifier** (AUC-ROC: 0.8345) - Adaptive boosting approach\n",
    "\n",
    "Each model will be optimized separately with SMOTE-Tomek class imbalance technique for ultra-fast execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e254d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ catboost already installed\n",
      "✅ optuna already installed\n",
      "✅ lightgbm already installed\n",
      "📦 Installing imbalanced-learn...\n",
      "✅ imbalanced-learn installed successfully\n",
      "\n",
      "📦 All required packages are now available!\n",
      "Available techniques:\n",
      "✓ SMOTE-ENN & SMOTE-Tomek for hybrid sampling\n",
      "✓ Optuna for Bayesian hyperparameter optimization\n",
      "✓ LightGBM for fast gradient boosting\n",
      "✓ Advanced ensemble methods\n",
      "✓ Class weighting and sampling capabilities\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for advanced techniques\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'catboost',\n",
    "    'optuna', \n",
    "    'lightgbm',\n",
    "    'imbalanced-learn'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✅ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "\n",
    "print(\"\\n📦 All required packages are now available!\")\n",
    "print(\"Available techniques:\")\n",
    "print(\"✓ SMOTE-ENN & SMOTE-Tomek for hybrid sampling\")\n",
    "print(\"✓ Optuna for Bayesian hyperparameter optimization\") \n",
    "print(\"✓ LightGBM for fast gradient boosting\")\n",
    "print(\"✓ Advanced ensemble methods\")\n",
    "print(\"✓ Class weighting and sampling capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe38ebd",
   "metadata": {},
   "source": [
    "## 🎯 Class Imbalance Handling Techniques\n",
    "\n",
    "We'll apply multiple class imbalance techniques to improve model performance:\n",
    "\n",
    "### 1. **Hybrid Sampling Methods**\n",
    "- **SMOTE-ENN**: Combines SMOTE oversampling with Edited Nearest Neighbours undersampling\n",
    "- **SMOTE-Tomek**: Combines SMOTE oversampling with Tomek Links undersampling\n",
    "\n",
    "### 2. **Algorithmic Approaches**\n",
    "- **Class Weights**: Automatically balance classes in model training\n",
    "- **Focal Loss**: Focus learning on hard-to-classify minority examples\n",
    "\n",
    "### 3. **Advanced Models**\n",
    "We'll test the top 5 performing algorithms:\n",
    "- **Gradient Boosting Classifier** (ensemble method with boosting)\n",
    "- **CatBoost Classifier** (gradient boosting with categorical features)\n",
    "- **AdaBoost Classifier** (adaptive boosting algorithm)\n",
    "- **LightGBM Classifier** (fast gradient boosting framework)\n",
    "- **Logistic Regression** (linear model with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f1c85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up class imbalance handling techniques...\n",
      "Class weights: {0: 0.6809927360774818, 1: 1.8812709030100334}\n",
      "Scale pos weight (XGBoost): 0.362\n",
      "✅ Class imbalance techniques and extended hyperparameters ready!\n"
     ]
    }
   ],
   "source": [
    "# Class Imbalance Handling Setup\n",
    "print(\"🔧 Setting up class imbalance handling techniques...\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "scale_pos_weight = class_weights[0] / class_weights[1]\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"Scale pos weight (XGBoost): {scale_pos_weight:.3f}\")\n",
    "\n",
    "# Initialize sampling techniques\n",
    "sampling_techniques = {\n",
    "    'original': None,\n",
    "    'smote_enn': SMOTEENN(random_state=42, n_jobs=-1),\n",
    "    'smote_tomek': SMOTETomek(random_state=42, n_jobs=-1),\n",
    "    'smote_only': SMOTE(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Function to apply sampling\n",
    "def apply_sampling(technique_name, X_train, y_train):\n",
    "    if technique_name == 'original':\n",
    "        return X_train, y_train\n",
    "    else:\n",
    "        technique = sampling_techniques[technique_name]\n",
    "        X_resampled, y_resampled = technique.fit_resample(X_train, y_train)\n",
    "        print(f\"  {technique_name}: {Counter(y_train)} → {Counter(y_resampled)}\")\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "# Extended hyperparameter search spaces for better performance\n",
    "hyperparameter_spaces = {\n",
    "    'gradientboosting': {\n",
    "        'n_estimators': [200, 300, 500, 700, 1000, 1500],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 10],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "        'max_features': ['sqrt', 'log2', 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 6, 8]\n",
    "    },\n",
    "    'catboost': {\n",
    "        'iterations': [200, 300, 500, 700, 1000, 1500],\n",
    "        'depth': [4, 5, 6, 7, 8, 9, 10, 12],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9, 12, 15, 20],\n",
    "        'border_count': [32, 64, 128, 200, 254],\n",
    "        'bagging_temperature': [0, 0.5, 1.0, 2.0, 3.0],\n",
    "        'random_strength': [0, 1, 2, 3, 5]\n",
    "    },\n",
    "    'adaboost': {\n",
    "        'n_estimators': [100, 200, 300, 500, 700, 1000, 1500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3, 0.5, 1.0, 1.5, 2.0],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'n_estimators': [200, 300, 500, 700, 1000, 1500],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 10, 12],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.3, 0.5, 1.0, 2.0],\n",
    "        'reg_lambda': [0.1, 0.3, 0.5, 1.0, 2.0, 3.0],\n",
    "        'min_child_samples': [5, 10, 20, 30, 40, 50],\n",
    "        'num_leaves': [31, 50, 70, 90, 110, 130]\n",
    "    },\n",
    "    'logisticregression': {\n",
    "        'C': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [1000, 2000, 3000, 5000],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # For elasticnet\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Class imbalance techniques and extended hyperparameters ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c55d8b",
   "metadata": {},
   "source": [
    "## 🔍 Advanced Model Optimization with Class Imbalance Handling\n",
    "\n",
    "We'll use Optuna Bayesian optimization for efficient hyperparameter search combined with SMOTE-Tomek class imbalance technique.\n",
    "Each model will be tested with:\n",
    "1. **SMOTE-Tomek** (hybrid sampling combining oversampling and undersampling)\n",
    "\n",
    "**Strategy**: \n",
    "- Optuna Bayesian optimization with **ultra-fast trial budget** (20 trials per model)\n",
    "- **Pruning**: Early stopping of underperforming trials (MedianPruner)\n",
    "- **Parallel processing**: Multi-core optimization (n_jobs=-1)\n",
    "- **Warm start**: Faster gradient boosting training\n",
    "- 5-fold Stratified Cross-Validation\n",
    "- Primary metrics: **AUC-ROC**, **Accuracy**, **Precision**, **Recall**, **F1-Score**\n",
    "- **SPEED OPTIMIZED** for fastest execution using single best sampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0052aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimized Optuna function ready with MAXIMUM SPEED!\n",
      "🚀 Features: Ultra-fast trial budgets, pruning, parallel processing, warm start\n",
      "📝 Trial budget: smote_tomek=20 (focused approach)\n"
     ]
    }
   ],
   "source": [
    "# Optimized Optuna-based Hyperparameter Tuning Function with Speed Improvements\n",
    "def optimize_model_with_optuna(model_name, model_class, param_space, X_train, y_train, \n",
    "                               sampling_technique='original', n_trials=200, class_weight_dict=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Optimize model hyperparameters using Optuna with class imbalance handling and speed optimizations\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Optimizing {model_name} with {sampling_technique} sampling...\")\n",
    "    \n",
    "    # Dynamic trial budget based on sampling technique complexity (SPEED OPTIMIZED)\n",
    "    trial_budget = {\n",
    "        'smote_tomek': 20,      # Focused on single best hybrid sampling technique\n",
    "    }\n",
    "    actual_trials = trial_budget.get(sampling_technique, 20)\n",
    "    print(f\"   Using {actual_trials} trials (optimized budget)\")\n",
    "    \n",
    "    # Apply sampling technique\n",
    "    X_resampled, y_resampled = apply_sampling(sampling_technique, X_train, y_train)\n",
    "    \n",
    "    # Create stratified cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Sample hyperparameters based on model type\n",
    "        params = {}\n",
    "        \n",
    "        if model_name == 'LogisticRegression':\n",
    "            params['C'] = trial.suggest_float('C', 0.01, 100.0, log=True)\n",
    "            penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
    "            params['penalty'] = penalty\n",
    "            \n",
    "            if penalty == 'elasticnet':\n",
    "                params['solver'] = 'saga'\n",
    "                params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "            elif penalty == 'l1':\n",
    "                params['solver'] = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            else:  # l2\n",
    "                params['solver'] = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            \n",
    "            params['max_iter'] = trial.suggest_categorical('max_iter', [1000, 2000, 3000, 5000])\n",
    "            \n",
    "        else:\n",
    "            # For tree-based models\n",
    "            for param, values in param_space.items():\n",
    "                if isinstance(values, list):\n",
    "                    if all(isinstance(v, int) for v in values):\n",
    "                        params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "                    elif all(isinstance(v, float) for v in values):\n",
    "                        params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "                    else:\n",
    "                        params[param] = trial.suggest_categorical(param, values)\n",
    "        \n",
    "        # Add class balancing parameters for original sampling\n",
    "        if sampling_technique == 'original':\n",
    "            if model_name == 'LogisticRegression':\n",
    "                params['class_weight'] = class_weight_dict\n",
    "            elif model_name == 'LightGBM':\n",
    "                params['class_weight'] = class_weight_dict\n",
    "            elif model_name == 'CatBoost':\n",
    "                params['class_weights'] = [class_weight_dict[0], class_weight_dict[1]]\n",
    "            # GradientBoosting and AdaBoost rely on sampling techniques\n",
    "        \n",
    "        # Add warm_start for gradient boosting models to speed up training\n",
    "        if model_name == 'GradientBoosting':\n",
    "            params['warm_start'] = True\n",
    "        \n",
    "        # Create model with sampled parameters\n",
    "        try:\n",
    "            if model_name == 'CatBoost':\n",
    "                model = model_class(**params, random_state=42, verbose=False)\n",
    "            elif model_name == 'LightGBM':\n",
    "                model = model_class(**params, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "            elif model_name == 'LogisticRegression':\n",
    "                model = model_class(**params, random_state=42, n_jobs=n_jobs)\n",
    "            else:\n",
    "                model = model_class(**params, random_state=42)\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "        \n",
    "        # Perform cross-validation with parallel processing\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, \n",
    "                                   scoring='roc_auc', n_jobs=n_jobs)\n",
    "            return scores.mean()\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "    \n",
    "    # Create study with pruning for early stopping of underperforming trials\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        study_name=f\"{model_name}_{sampling_technique}\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
    "    )\n",
    "    \n",
    "    # Run optimization with parallel trials\n",
    "    study.optimize(objective, n_trials=actual_trials, show_progress_bar=True, n_jobs=min(4, n_jobs) if n_jobs > 0 else 4)\n",
    "    \n",
    "    # Get best parameters and create best model\n",
    "    best_params = study.best_params.copy()\n",
    "    \n",
    "    # Add class balancing to best params for original sampling\n",
    "    if sampling_technique == 'original':\n",
    "        if model_name == 'LogisticRegression':\n",
    "            best_params['class_weight'] = class_weight_dict\n",
    "        elif model_name == 'LightGBM':\n",
    "            best_params['class_weight'] = class_weight_dict\n",
    "        elif model_name == 'CatBoost':\n",
    "            best_params['class_weights'] = [class_weight_dict[0], class_weight_dict[1]]\n",
    "    \n",
    "    # Remove warm_start from final model params as it's only for training optimization\n",
    "    if 'warm_start' in best_params:\n",
    "        del best_params['warm_start']\n",
    "    \n",
    "    # Create and evaluate best model\n",
    "    if model_name == 'CatBoost':\n",
    "        best_model = model_class(**best_params, random_state=42, verbose=False)\n",
    "    elif model_name == 'LightGBM':\n",
    "        best_model = model_class(**best_params, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "    elif model_name == 'LogisticRegression':\n",
    "        best_model = model_class(**best_params, random_state=42, n_jobs=n_jobs)\n",
    "    else:\n",
    "        best_model = model_class(**best_params, random_state=42)\n",
    "    \n",
    "    # Get comprehensive cross-validation scores with parallel processing\n",
    "    cv_results = {}\n",
    "    for metric in ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']:\n",
    "        scores = cross_val_score(best_model, X_resampled, y_resampled, cv=cv, \n",
    "                               scoring=metric, n_jobs=n_jobs)\n",
    "        cv_results[metric] = {'mean': scores.mean(), 'std': scores.std()}\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'best_params': best_params,\n",
    "        'best_score': study.best_value,\n",
    "        'cv_results': cv_results,\n",
    "        'study': study,\n",
    "        'sampling_data': (X_resampled, y_resampled),\n",
    "        'trials_used': actual_trials,\n",
    "        'pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])\n",
    "    }\n",
    "\n",
    "print(\"✅ Optimized Optuna function ready with MAXIMUM SPEED!\")\n",
    "print(\"🚀 Features: Ultra-fast trial budgets, pruning, parallel processing, warm start\")\n",
    "print(\"📝 Trial budget: smote_tomek=20 (focused approach)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c2a39",
   "metadata": {},
   "source": [
    "## 🎯 Model 1: Gradient Boosting Classifier Optimization\n",
    "\n",
    "Optimizing the best performing model from baseline analysis with comprehensive class imbalance techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714923e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 15:53:21,431] A new study created in memory with name: GradientBoosting_smote_tomek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 OPTIMIZING GRADIENT BOOSTING CLASSIFIER\n",
      "============================================================\n",
      "\n",
      "🔍 Optimizing GradientBoosting with smote_tomek sampling...\n",
      "   Using 20 trials (optimized budget)\n",
      "  smote_tomek: Counter({0: 3304, 1: 1196}) → Counter({0: 2929, 1: 2929})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d58e83c4e454967aa9992ce477b8900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 15:53:31,034] Trial 1 finished with value: 0.9377457303536232 and parameters: {'n_estimators': 221, 'max_depth': 3, 'learning_rate': 0.055161648382533686, 'subsample': 0.9294052864169349, 'max_features': 0.8, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9377457303536232.\n",
      "[I 2025-07-01 15:53:42,145] Trial 4 finished with value: 0.938593382457551 and parameters: {'n_estimators': 360, 'max_depth': 4, 'learning_rate': 0.2997322374557249, 'subsample': 0.7947967106684836, 'max_features': 0.8, 'min_samples_split': 15, 'min_samples_leaf': 3}. Best is trial 4 with value: 0.938593382457551.\n",
      "[I 2025-07-01 15:53:53,315] Trial 2 finished with value: 0.9359169747757268 and parameters: {'n_estimators': 1326, 'max_depth': 10, 'learning_rate': 0.11311666281124891, 'subsample': 0.8398590991628633, 'max_features': 'sqrt', 'min_samples_split': 13, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.938593382457551.\n",
      "[I 2025-07-01 15:54:01,167] Trial 6 finished with value: 0.9409543988635184 and parameters: {'n_estimators': 824, 'max_depth': 4, 'learning_rate': 0.17446237511680956, 'subsample': 0.8136271549552394, 'max_features': 'log2', 'min_samples_split': 18, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.9409543988635184.\n",
      "[I 2025-07-01 15:54:05,052] Trial 3 finished with value: 0.9388092594102566 and parameters: {'n_estimators': 911, 'max_depth': 6, 'learning_rate': 0.23606129880996987, 'subsample': 0.8699117098142266, 'max_features': 0.8, 'min_samples_split': 16, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.9409543988635184.\n",
      "[I 2025-07-01 15:54:15,687] Trial 0 finished with value: 0.9393014463976851 and parameters: {'n_estimators': 1340, 'max_depth': 6, 'learning_rate': 0.1115123156383752, 'subsample': 0.8500546324816503, 'max_features': 0.7, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.9409543988635184.\n",
      "[I 2025-07-01 15:54:17,167] Trial 7 finished with value: 0.9404303585912575 and parameters: {'n_estimators': 247, 'max_depth': 8, 'learning_rate': 0.18846131000490865, 'subsample': 0.9683702180493197, 'max_features': 0.8, 'min_samples_split': 17, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.9409543988635184.\n",
      "[I 2025-07-01 15:54:21,881] Trial 8 finished with value: 0.9408139868520886 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.09275946373339898, 'subsample': 0.7779582249538183, 'max_features': 1.0, 'min_samples_split': 15, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.9409543988635184.\n",
      "[I 2025-07-01 15:54:30,425] Trial 10 finished with value: 0.9441895405100567 and parameters: {'n_estimators': 619, 'max_depth': 9, 'learning_rate': 0.034242032933247527, 'subsample': 0.894080635860054, 'max_features': 'log2', 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:54:35,490] Trial 11 finished with value: 0.942098752600098 and parameters: {'n_estimators': 236, 'max_depth': 8, 'learning_rate': 0.11941695497033228, 'subsample': 0.7609540233461963, 'max_features': 0.9, 'min_samples_split': 16, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:54:49,262] Trial 13 finished with value: 0.9424845039486446 and parameters: {'n_estimators': 556, 'max_depth': 10, 'learning_rate': 0.011857723011748676, 'subsample': 0.7024769540483798, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:54:50,351] Trial 5 finished with value: 0.9400467263480812 and parameters: {'n_estimators': 1098, 'max_depth': 10, 'learning_rate': 0.048654135157940676, 'subsample': 0.8839573192643944, 'max_features': 0.7, 'min_samples_split': 17, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:54:51,670] Trial 12 finished with value: 0.9417922424515396 and parameters: {'n_estimators': 834, 'max_depth': 4, 'learning_rate': 0.0690451009457974, 'subsample': 0.7268660064565574, 'max_features': 0.8, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:54:58,472] Trial 9 finished with value: 0.9350295206262078 and parameters: {'n_estimators': 1048, 'max_depth': 6, 'learning_rate': 0.27170146288113195, 'subsample': 0.9219991716732796, 'max_features': 0.7, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:55:00,385] Trial 15 finished with value: 0.9417986465602046 and parameters: {'n_estimators': 544, 'max_depth': 9, 'learning_rate': 0.010501406916338135, 'subsample': 0.7077531707289709, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:55:01,643] Trial 16 finished with value: 0.9420621603246329 and parameters: {'n_estimators': 544, 'max_depth': 9, 'learning_rate': 0.012781897681521135, 'subsample': 0.7013914583557408, 'max_features': 'log2', 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:55:02,114] Trial 14 finished with value: 0.9428733804424843 and parameters: {'n_estimators': 648, 'max_depth': 10, 'learning_rate': 0.013610883509292286, 'subsample': 0.7049646523463702, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:55:08,078] Trial 17 finished with value: 0.9419330755959605 and parameters: {'n_estimators': 527, 'max_depth': 9, 'learning_rate': 0.010381178604660504, 'subsample': 0.7227460783904538, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9441895405100567.\n",
      "[I 2025-07-01 15:55:10,546] Trial 19 finished with value: 0.9444417548681431 and parameters: {'n_estimators': 506, 'max_depth': 8, 'learning_rate': 0.037662183456093366, 'subsample': 0.9823743551045159, 'max_features': 'log2', 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 19 with value: 0.9444417548681431.\n",
      "[I 2025-07-01 15:55:11,162] Trial 18 finished with value: 0.9422960832740213 and parameters: {'n_estimators': 580, 'max_depth': 8, 'learning_rate': 0.010592319676466833, 'subsample': 0.9900006421461751, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 19 with value: 0.9444417548681431.\n",
      "\n",
      "✅ SMOTE_TOMEK:\n",
      "   AUC-ROC: 0.9444 (±0.0024)\n",
      "   Accuracy: 0.8709 (±0.0074)\n",
      "   Precision: 0.8632 (±0.0051)\n",
      "   Recall: 0.8815 (±0.0115)\n",
      "   F1-Score: 0.8723 (±0.0078)\n",
      "   Trials: 20 | Pruned: 0 | Time: 151.3s\n",
      "\n",
      "🏆 BEST GRADIENT BOOSTING CONFIGURATION:\n",
      "Sampling Method: smote_tomek\n",
      "Best Accuracy: 0.8709\n",
      "Best AUC-ROC: 0.9444\n",
      "Total time: 2.5 minutes\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Gradient Boosting Classifier\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🚀 OPTIMIZING GRADIENT BOOSTING CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define model and sampling techniques\n",
    "model_name = 'GradientBoosting'\n",
    "model_class = GradientBoostingClassifier\n",
    "sampling_methods = ['smote_tomek']  # Using only SMOTE-Tomek for speed\n",
    "\n",
    "# Store results for this model\n",
    "gb_results = {}\n",
    "gb_times = {}\n",
    "\n",
    "for sampling_method in sampling_methods:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get hyperparameter space\n",
    "        param_space = hyperparameter_spaces['gradientboosting']\n",
    "        \n",
    "        # Run optimization with speed improvements\n",
    "        result = optimize_model_with_optuna(\n",
    "            model_name=model_name,\n",
    "            model_class=model_class,\n",
    "            param_space=param_space,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            sampling_technique=sampling_method,\n",
    "            class_weight_dict=class_weight_dict,\n",
    "            n_jobs=-1  # Use all available CPU cores\n",
    "        )\n",
    "        \n",
    "        gb_results[sampling_method] = result\n",
    "        gb_times[sampling_method] = time.time() - start_time\n",
    "        \n",
    "        # Print results with optimization stats\n",
    "        cv_results = result['cv_results']\n",
    "        print(f\"\\n✅ {sampling_method.upper()}:\")\n",
    "        print(f\"   AUC-ROC: {cv_results['roc_auc']['mean']:.4f} (±{cv_results['roc_auc']['std']:.4f})\")\n",
    "        print(f\"   Accuracy: {cv_results['accuracy']['mean']:.4f} (±{cv_results['accuracy']['std']:.4f})\")\n",
    "        print(f\"   Precision: {cv_results['precision']['mean']:.4f} (±{cv_results['precision']['std']:.4f})\")\n",
    "        print(f\"   Recall: {cv_results['recall']['mean']:.4f} (±{cv_results['recall']['std']:.4f})\")\n",
    "        print(f\"   F1-Score: {cv_results['f1']['mean']:.4f} (±{cv_results['f1']['std']:.4f})\")\n",
    "        print(f\"   Trials: {result['trials_used']} | Pruned: {result['pruned_trials']} | Time: {gb_times[sampling_method]:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {sampling_method}: Optimization failed - {str(e)}\")\n",
    "        gb_results[sampling_method] = None\n",
    "        gb_times[sampling_method] = time.time() - start_time\n",
    "\n",
    "# Find best configuration for Gradient Boosting\n",
    "best_gb_method = max([k for k, v in gb_results.items() if v is not None], \n",
    "                     key=lambda x: gb_results[x]['cv_results']['accuracy']['mean'])\n",
    "best_gb_result = gb_results[best_gb_method]\n",
    "\n",
    "print(f\"\\n🏆 BEST GRADIENT BOOSTING CONFIGURATION:\")\n",
    "print(f\"Sampling Method: {best_gb_method}\")\n",
    "print(f\"Best Accuracy: {best_gb_result['cv_results']['accuracy']['mean']:.4f}\")\n",
    "print(f\"Best AUC-ROC: {best_gb_result['cv_results']['roc_auc']['mean']:.4f}\")\n",
    "print(f\"Total time: {sum(gb_times.values())/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d138144",
   "metadata": {},
   "source": [
    "## 🎯 Model 2: CatBoost Classifier Optimization\n",
    "\n",
    "Optimizing the second-best performing model with advanced categorical feature handling and gradient boosting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58160fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 15:55:52,775] A new study created in memory with name: CatBoost_smote_tomek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 OPTIMIZING CATBOOST CLASSIFIER\n",
      "============================================================\n",
      "\n",
      "🔍 Optimizing CatBoost with smote_tomek sampling...\n",
      "   Using 20 trials (optimized budget)\n",
      "  smote_tomek: Counter({0: 3304, 1: 1196}) → Counter({0: 2929, 1: 2929})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79f0c2a596d4f1986dde2a58339112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 15:56:36,521] Trial 3 finished with value: 0.9398791124308602 and parameters: {'iterations': 942, 'depth': 6, 'learning_rate': 0.20917106003129393, 'l2_leaf_reg': 8, 'border_count': 141, 'bagging_temperature': 0, 'random_strength': 0}. Best is trial 3 with value: 0.9398791124308602.\n",
      "[I 2025-07-01 15:56:48,571] Trial 2 finished with value: 0.9409294235864143 and parameters: {'iterations': 658, 'depth': 8, 'learning_rate': 0.26204081307778715, 'l2_leaf_reg': 16, 'border_count': 124, 'bagging_temperature': 2.0, 'random_strength': 5}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 15:57:03,795] Trial 1 finished with value: 0.9395947645304072 and parameters: {'iterations': 612, 'depth': 10, 'learning_rate': 0.22810010209863743, 'l2_leaf_reg': 5, 'border_count': 61, 'bagging_temperature': 3.0, 'random_strength': 0}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 15:57:32,218] Trial 4 finished with value: 0.9377502493196911 and parameters: {'iterations': 1497, 'depth': 5, 'learning_rate': 0.23049829345519088, 'l2_leaf_reg': 1, 'border_count': 191, 'bagging_temperature': 3.0, 'random_strength': 1}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 15:57:35,608] Trial 6 finished with value: 0.9401124121432453 and parameters: {'iterations': 666, 'depth': 7, 'learning_rate': 0.2004584036620637, 'l2_leaf_reg': 11, 'border_count': 76, 'bagging_temperature': 1.0, 'random_strength': 2}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 15:59:00,755] Trial 7 finished with value: 0.9386125947835462 and parameters: {'iterations': 841, 'depth': 8, 'learning_rate': 0.28149653507763167, 'l2_leaf_reg': 1, 'border_count': 104, 'bagging_temperature': 0, 'random_strength': 5}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 15:59:58,680] Trial 5 finished with value: 0.9407178376105211 and parameters: {'iterations': 1052, 'depth': 10, 'learning_rate': 0.14654342838126802, 'l2_leaf_reg': 11, 'border_count': 78, 'bagging_temperature': 3.0, 'random_strength': 3}. Best is trial 2 with value: 0.9409294235864143.\n",
      "[I 2025-07-01 16:00:16,525] Trial 0 finished with value: 0.9419053375669645 and parameters: {'iterations': 862, 'depth': 12, 'learning_rate': 0.061071707962992695, 'l2_leaf_reg': 3, 'border_count': 118, 'bagging_temperature': 0, 'random_strength': 3}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:00:30,597] Trial 11 finished with value: 0.9327730517297665 and parameters: {'iterations': 557, 'depth': 4, 'learning_rate': 0.016619082853255817, 'l2_leaf_reg': 19, 'border_count': 181, 'bagging_temperature': 1.0, 'random_strength': 1}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:00:36,663] Trial 9 finished with value: 0.9414950549727891 and parameters: {'iterations': 1117, 'depth': 6, 'learning_rate': 0.1789830954515734, 'l2_leaf_reg': 20, 'border_count': 240, 'bagging_temperature': 3.0, 'random_strength': 0}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:01:39,374] Trial 13 finished with value: 0.9406982106228459 and parameters: {'iterations': 200, 'depth': 12, 'learning_rate': 0.07925156224812623, 'l2_leaf_reg': 6, 'border_count': 43, 'bagging_temperature': 0.5, 'random_strength': 3}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:02:15,593] Trial 10 finished with value: 0.9418955385091274 and parameters: {'iterations': 680, 'depth': 9, 'learning_rate': 0.07609418779081771, 'l2_leaf_reg': 4, 'border_count': 145, 'bagging_temperature': 1.0, 'random_strength': 1}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:05:03,755] Trial 12 finished with value: 0.9416009519995006 and parameters: {'iterations': 672, 'depth': 10, 'learning_rate': 0.036646591526838905, 'l2_leaf_reg': 11, 'border_count': 219, 'bagging_temperature': 2.0, 'random_strength': 5}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:05:39,447] Trial 8 finished with value: 0.9375142391236929 and parameters: {'iterations': 806, 'depth': 12, 'learning_rate': 0.01363550440015793, 'l2_leaf_reg': 19, 'border_count': 224, 'bagging_temperature': 0.5, 'random_strength': 1}. Best is trial 0 with value: 0.9419053375669645.\n",
      "[I 2025-07-01 16:06:59,745] Trial 15 finished with value: 0.9421009304450578 and parameters: {'iterations': 395, 'depth': 12, 'learning_rate': 0.0908927291794112, 'l2_leaf_reg': 4, 'border_count': 153, 'bagging_temperature': 1.0, 'random_strength': 4}. Best is trial 15 with value: 0.9421009304450578.\n",
      "[I 2025-07-01 16:08:32,107] Trial 16 finished with value: 0.9415131562245099 and parameters: {'iterations': 271, 'depth': 12, 'learning_rate': 0.11212787374440872, 'l2_leaf_reg': 4, 'border_count': 167, 'bagging_temperature': 1.0, 'random_strength': 2}. Best is trial 15 with value: 0.9421009304450578.\n",
      "[I 2025-07-01 16:09:40,063] Trial 18 finished with value: 0.941722765463869 and parameters: {'iterations': 233, 'depth': 12, 'learning_rate': 0.12299803590381614, 'l2_leaf_reg': 3, 'border_count': 162, 'bagging_temperature': 0, 'random_strength': 4}. Best is trial 15 with value: 0.9421009304450578.\n",
      "[I 2025-07-01 16:10:52,342] Trial 19 finished with value: 0.9414134850965217 and parameters: {'iterations': 406, 'depth': 11, 'learning_rate': 0.11565293958096924, 'l2_leaf_reg': 7, 'border_count': 104, 'bagging_temperature': 0, 'random_strength': 4}. Best is trial 15 with value: 0.9421009304450578.\n",
      "[I 2025-07-01 16:13:36,109] Trial 17 finished with value: 0.9408327407106076 and parameters: {'iterations': 1238, 'depth': 11, 'learning_rate': 0.08102774570776522, 'l2_leaf_reg': 4, 'border_count': 165, 'bagging_temperature': 1.0, 'random_strength': 4}. Best is trial 15 with value: 0.9421009304450578.\n",
      "[I 2025-07-01 16:14:22,343] Trial 14 finished with value: 0.9424360134216976 and parameters: {'iterations': 1299, 'depth': 12, 'learning_rate': 0.1285139843996024, 'l2_leaf_reg': 16, 'border_count': 242, 'bagging_temperature': 0, 'random_strength': 4}. Best is trial 14 with value: 0.9424360134216976.\n",
      "\n",
      "✅ SMOTE_TOMEK:\n",
      "   AUC-ROC: 0.9424 (±0.0030)\n",
      "   Accuracy: 0.8670 (±0.0075)\n",
      "   Precision: 0.8637 (±0.0086)\n",
      "   Recall: 0.8716 (±0.0124)\n",
      "   F1-Score: 0.8676 (±0.0078)\n",
      "   Trials: 20 | Pruned: 0 | Time: 2904.8s\n",
      "\n",
      "🏆 BEST CATBOOST CONFIGURATION:\n",
      "Sampling Method: smote_tomek\n",
      "Best Accuracy: 0.8670\n",
      "Best AUC-ROC: 0.9424\n",
      "Total time: 48.4 minutes\n"
     ]
    }
   ],
   "source": [
    "# Model 2: CatBoost Classifier\n",
    "print(\"🚀 OPTIMIZING CATBOOST CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define model and sampling techniques\n",
    "model_name = 'CatBoost'\n",
    "model_class = CatBoostClassifier\n",
    "sampling_methods = ['smote_tomek']  # Using only SMOTE-Tomek for speed\n",
    "\n",
    "# Store results for this model\n",
    "catboost_results = {}\n",
    "catboost_times = {}\n",
    "\n",
    "for sampling_method in sampling_methods:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get hyperparameter space\n",
    "        param_space = hyperparameter_spaces['catboost']\n",
    "        \n",
    "        # Run optimization with speed improvements\n",
    "        result = optimize_model_with_optuna(\n",
    "            model_name=model_name,\n",
    "            model_class=model_class,\n",
    "            param_space=param_space,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            sampling_technique=sampling_method,\n",
    "            class_weight_dict=class_weight_dict,\n",
    "            n_jobs=-1  # Use all available CPU cores\n",
    "        )\n",
    "        \n",
    "        catboost_results[sampling_method] = result\n",
    "        catboost_times[sampling_method] = time.time() - start_time\n",
    "        \n",
    "        # Print results with optimization stats\n",
    "        cv_results = result['cv_results']\n",
    "        print(f\"\\n✅ {sampling_method.upper()}:\")\n",
    "        print(f\"   AUC-ROC: {cv_results['roc_auc']['mean']:.4f} (±{cv_results['roc_auc']['std']:.4f})\")\n",
    "        print(f\"   Accuracy: {cv_results['accuracy']['mean']:.4f} (±{cv_results['accuracy']['std']:.4f})\")\n",
    "        print(f\"   Precision: {cv_results['precision']['mean']:.4f} (±{cv_results['precision']['std']:.4f})\")\n",
    "        print(f\"   Recall: {cv_results['recall']['mean']:.4f} (±{cv_results['recall']['std']:.4f})\")\n",
    "        print(f\"   F1-Score: {cv_results['f1']['mean']:.4f} (±{cv_results['f1']['std']:.4f})\")\n",
    "        print(f\"   Trials: {result['trials_used']} | Pruned: {result['pruned_trials']} | Time: {catboost_times[sampling_method]:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {sampling_method}: Optimization failed - {str(e)}\")\n",
    "        catboost_results[sampling_method] = None\n",
    "        catboost_times[sampling_method] = time.time() - start_time\n",
    "\n",
    "# Find best configuration for CatBoost\n",
    "best_catboost_method = max([k for k, v in catboost_results.items() if v is not None], \n",
    "                          key=lambda x: catboost_results[x]['cv_results']['accuracy']['mean'])\n",
    "best_catboost_result = catboost_results[best_catboost_method]\n",
    "\n",
    "print(f\"\\n🏆 BEST CATBOOST CONFIGURATION:\")\n",
    "print(f\"Sampling Method: {best_catboost_method}\")\n",
    "print(f\"Best Accuracy: {best_catboost_result['cv_results']['accuracy']['mean']:.4f}\")\n",
    "print(f\"Best AUC-ROC: {best_catboost_result['cv_results']['roc_auc']['mean']:.4f}\")\n",
    "print(f\"Total time: {sum(catboost_times.values())/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d70cdb",
   "metadata": {},
   "source": [
    "## 🎯 Model 3: AdaBoost Classifier Optimization\n",
    "\n",
    "Optimizing the adaptive boosting algorithm with advanced class imbalance handling for improved minority class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d8fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 16:44:17,619] A new study created in memory with name: AdaBoost_smote_tomek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 OPTIMIZING ADABOOST CLASSIFIER\n",
      "============================================================\n",
      "\n",
      "🔍 Optimizing AdaBoost with smote_tomek sampling...\n",
      "   Using 20 trials (optimized budget)\n",
      "  smote_tomek: Counter({0: 3304, 1: 1196}) → Counter({0: 2929, 1: 2929})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c736130a4c03487c9cc5549c26329bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-01 16:44:25,263] Trial 2 finished with value: 0.9349729519122872 and parameters: {'n_estimators': 383, 'learning_rate': 1.6198964781600413, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:29,189] Trial 4 finished with value: 0.9292149244753236 and parameters: {'n_estimators': 283, 'learning_rate': 1.2103830686285866, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:30,252] Trial 3 finished with value: 0.9333498301977843 and parameters: {'n_estimators': 781, 'learning_rate': 1.2845599554892917, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:31,525] Trial 0 finished with value: 0.933031425282296 and parameters: {'n_estimators': 872, 'learning_rate': 1.283889166562746, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:39,299] Trial 1 finished with value: 0.932570891964653 and parameters: {'n_estimators': 1445, 'learning_rate': 1.7310200756766814, 'algorithm': 'SAMME.R'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:44,582] Trial 8 finished with value: 0.9158462518118426 and parameters: {'n_estimators': 362, 'learning_rate': 0.27566282487977656, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:47,516] Trial 7 finished with value: 0.9343111717821383 and parameters: {'n_estimators': 1105, 'learning_rate': 0.958971436014522, 'algorithm': 'SAMME.R'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:48,061] Trial 6 finished with value: 0.9325511614360057 and parameters: {'n_estimators': 1383, 'learning_rate': 1.8898727570428497, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:48,362] Trial 5 finished with value: 0.9348695025344143 and parameters: {'n_estimators': 1460, 'learning_rate': 1.728693105917484, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:49,634] Trial 9 finished with value: 0.9340128040358678 and parameters: {'n_estimators': 357, 'learning_rate': 1.547940973034539, 'algorithm': 'SAMME.R'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:58,174] Trial 13 finished with value: 0.9345540236469658 and parameters: {'n_estimators': 604, 'learning_rate': 0.7152927752711121, 'algorithm': 'SAMME.R'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:59,379] Trial 11 finished with value: 0.9299428254718356 and parameters: {'n_estimators': 847, 'learning_rate': 1.0144202647822838, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:44:59,670] Trial 14 finished with value: 0.9135793745587686 and parameters: {'n_estimators': 111, 'learning_rate': 1.9593711144609043, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:45:01,499] Trial 15 finished with value: 0.9226915973415059 and parameters: {'n_estimators': 154, 'learning_rate': 1.9460955538011309, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:45:05,298] Trial 12 finished with value: 0.9332907545976422 and parameters: {'n_estimators': 1277, 'learning_rate': 1.1925261440695452, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:45:05,977] Trial 10 finished with value: 0.9192442400107603 and parameters: {'n_estimators': 1335, 'learning_rate': 0.24261229138876408, 'algorithm': 'SAMME'}. Best is trial 2 with value: 0.9349729519122872.\n",
      "[I 2025-07-01 16:45:14,762] Trial 18 finished with value: 0.9356624013759799 and parameters: {'n_estimators': 587, 'learning_rate': 1.5592931505871277, 'algorithm': 'SAMME'}. Best is trial 18 with value: 0.9356624013759799.\n",
      "[I 2025-07-01 16:45:15,132] Trial 19 finished with value: 0.9355007878571927 and parameters: {'n_estimators': 579, 'learning_rate': 1.5549761773215711, 'algorithm': 'SAMME'}. Best is trial 18 with value: 0.9356624013759799.\n",
      "[I 2025-07-01 16:45:16,090] Trial 17 finished with value: 0.9367797139939507 and parameters: {'n_estimators': 1089, 'learning_rate': 1.579091444163584, 'algorithm': 'SAMME'}. Best is trial 17 with value: 0.9367797139939507.\n",
      "[I 2025-07-01 16:45:16,593] Trial 16 finished with value: 0.9364170975812929 and parameters: {'n_estimators': 1150, 'learning_rate': 1.5454723228506317, 'algorithm': 'SAMME'}. Best is trial 17 with value: 0.9367797139939507.\n",
      "\n",
      "✅ SMOTE_TOMEK:\n",
      "   AUC-ROC: 0.9368 (±0.0027)\n",
      "   Accuracy: 0.8532 (±0.0043)\n",
      "   Precision: 0.8369 (±0.0031)\n",
      "   Recall: 0.8774 (±0.0061)\n",
      "   F1-Score: 0.8567 (±0.0043)\n",
      "   Trials: 20 | Pruned: 0 | Time: 116.5s\n",
      "\n",
      "🏆 BEST ADABOOST CONFIGURATION:\n",
      "Sampling Method: smote_tomek\n",
      "Best Accuracy: 0.8532\n",
      "Best AUC-ROC: 0.9368\n",
      "Total time: 1.9 minutes\n"
     ]
    }
   ],
   "source": [
    "# Model 3: AdaBoost Classifier\n",
    "print(\"🚀 OPTIMIZING ADABOOST CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define model and sampling techniques\n",
    "model_name = 'AdaBoost'\n",
    "model_class = AdaBoostClassifier\n",
    "sampling_methods = ['smote_tomek']  # Using only SMOTE-Tomek for speed\n",
    "\n",
    "# Store results for this model\n",
    "adaboost_results = {}\n",
    "adaboost_times = {}\n",
    "\n",
    "for sampling_method in sampling_methods:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get hyperparameter space\n",
    "        param_space = hyperparameter_spaces['adaboost']\n",
    "        \n",
    "        # Run optimization with speed improvements\n",
    "        result = optimize_model_with_optuna(\n",
    "            model_name=model_name,\n",
    "            model_class=model_class,\n",
    "            param_space=param_space,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            sampling_technique=sampling_method,\n",
    "            class_weight_dict=class_weight_dict,\n",
    "            n_jobs=-1  # Use all available CPU cores\n",
    "        )\n",
    "        \n",
    "        adaboost_results[sampling_method] = result\n",
    "        adaboost_times[sampling_method] = time.time() - start_time\n",
    "        \n",
    "        # Print results with optimization stats\n",
    "        cv_results = result['cv_results']\n",
    "        print(f\"\\n✅ {sampling_method.upper()}:\")\n",
    "        print(f\"   AUC-ROC: {cv_results['roc_auc']['mean']:.4f} (±{cv_results['roc_auc']['std']:.4f})\")\n",
    "        print(f\"   Accuracy: {cv_results['accuracy']['mean']:.4f} (±{cv_results['accuracy']['std']:.4f})\")\n",
    "        print(f\"   Precision: {cv_results['precision']['mean']:.4f} (±{cv_results['precision']['std']:.4f})\")\n",
    "        print(f\"   Recall: {cv_results['recall']['mean']:.4f} (±{cv_results['recall']['std']:.4f})\")\n",
    "        print(f\"   F1-Score: {cv_results['f1']['mean']:.4f} (±{cv_results['f1']['std']:.4f})\")\n",
    "        print(f\"   Trials: {result['trials_used']} | Pruned: {result['pruned_trials']} | Time: {adaboost_times[sampling_method]:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {sampling_method}: Optimization failed - {str(e)}\")\n",
    "        adaboost_results[sampling_method] = None\n",
    "        adaboost_times[sampling_method] = time.time() - start_time\n",
    "\n",
    "# Find best configuration for AdaBoost\n",
    "best_adaboost_method = max([k for k, v in adaboost_results.items() if v is not None], \n",
    "                          key=lambda x: adaboost_results[x]['cv_results']['accuracy']['mean'])\n",
    "best_adaboost_result = adaboost_results[best_adaboost_method]\n",
    "\n",
    "print(f\"\\n🏆 BEST ADABOOST CONFIGURATION:\")\n",
    "print(f\"Sampling Method: {best_adaboost_method}\")\n",
    "print(f\"Best Accuracy: {best_adaboost_result['cv_results']['accuracy']['mean']:.4f}\")\n",
    "print(f\"Best AUC-ROC: {best_adaboost_result['cv_results']['roc_auc']['mean']:.4f}\")\n",
    "print(f\"Total time: {sum(adaboost_times.values())/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad152c1",
   "metadata": {},
   "source": [
    "## 📊 Comprehensive Results Analysis & Model Comparison\n",
    "\n",
    "Analyzing results from top 3 models with SMOTE-Tomek sampling (3 total configurations) to identify the best performer and create ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955464bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 TOP PERFORMING CONFIGURATIONS:\n",
      "====================================================================================================\n",
      "GradientBoosting   + smote_tomek  | Acc: 0.8709 (±0.0074) | AUC: 0.9444 (±0.0024) | F1: 0.8723 | Time: 151s\n",
      "CatBoost           + smote_tomek  | Acc: 0.8670 (±0.0075) | AUC: 0.9424 (±0.0030) | F1: 0.8676 | Time: 2905s\n",
      "AdaBoost           + smote_tomek  | Acc: 0.8532 (±0.0043) | AUC: 0.9368 (±0.0027) | F1: 0.8567 | Time: 116s\n",
      "\n",
      "🎯 BEST INDIVIDUAL MODEL CONFIGURATION:\n",
      "Model: GradientBoosting\n",
      "Sampling: smote_tomek\n",
      "Accuracy: 0.8709 ± 0.0074\n",
      "AUC-ROC: 0.9444 ± 0.0024\n",
      "Precision: 0.8632 ± 0.0051\n",
      "Recall: 0.8815 ± 0.0115\n",
      "F1-Score: 0.8723 ± 0.0078\n",
      "Training Time: 151.3 seconds\n",
      "\n",
      "📈 BEST CONFIGURATION FOR EACH MODEL:\n",
      "================================================================================\n",
      "GradientBoosting  : smote_tomek  | Acc: 0.8709 | AUC: 0.9444\n",
      "CatBoost          : smote_tomek  | Acc: 0.8670 | AUC: 0.9424\n",
      "AdaBoost          : smote_tomek  | Acc: 0.8532 | AUC: 0.9368\n",
      "\n",
      "💾 Results saved to advanced_optimization_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Consolidate all results from first 3 models (optimized for speed)\n",
    "all_results = {\n",
    "    'GradientBoosting': gb_results,\n",
    "    'CatBoost': catboost_results,\n",
    "    'AdaBoost': adaboost_results\n",
    "}\n",
    "\n",
    "optimization_times = {\n",
    "    'GradientBoosting': gb_times,\n",
    "    'CatBoost': catboost_times,\n",
    "    'AdaBoost': adaboost_times\n",
    "}\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_data = []\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    for sampling_method, result in model_results.items():\n",
    "        if result is not None:\n",
    "            cv_results = result['cv_results']\n",
    "            results_data.append({\n",
    "                'Model': model_name,\n",
    "                'Sampling': sampling_method,\n",
    "                'AUC-ROC': cv_results['roc_auc']['mean'],\n",
    "                'AUC-ROC_std': cv_results['roc_auc']['std'],\n",
    "                'Accuracy': cv_results['accuracy']['mean'],\n",
    "                'Accuracy_std': cv_results['accuracy']['std'],\n",
    "                'Precision': cv_results['precision']['mean'],\n",
    "                'Precision_std': cv_results['precision']['std'],\n",
    "                'Recall': cv_results['recall']['mean'],\n",
    "                'Recall_std': cv_results['recall']['std'],\n",
    "                'F1': cv_results['f1']['mean'],\n",
    "                'F1_std': cv_results['f1']['std'],\n",
    "                'Training_Time': optimization_times[model_name][sampling_method],\n",
    "                'Best_Params': str(result['best_params'])\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Sort by accuracy (primary metric)\n",
    "results_df_sorted = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"🏆 TOP PERFORMING CONFIGURATIONS:\")\n",
    "print(\"=\"*100)\n",
    "top_configs = results_df_sorted.head(3)  # Show all configurations from 3 models\n",
    "for idx, row in top_configs.iterrows():\n",
    "    print(f\"{row['Model']:18} + {row['Sampling']:12} | \"\n",
    "          f\"Acc: {row['Accuracy']:.4f} (±{row['Accuracy_std']:.4f}) | \"\n",
    "          f\"AUC: {row['AUC-ROC']:.4f} (±{row['AUC-ROC_std']:.4f}) | \"\n",
    "          f\"F1: {row['F1']:.4f} | \"\n",
    "          f\"Time: {row['Training_Time']:.0f}s\")\n",
    "\n",
    "# Find best overall configuration\n",
    "best_config = results_df_sorted.iloc[0]\n",
    "print(f\"\\n🎯 BEST INDIVIDUAL MODEL CONFIGURATION:\")\n",
    "print(f\"Model: {best_config['Model']}\")\n",
    "print(f\"Sampling: {best_config['Sampling']}\")\n",
    "print(f\"Accuracy: {best_config['Accuracy']:.4f} ± {best_config['Accuracy_std']:.4f}\")\n",
    "print(f\"AUC-ROC: {best_config['AUC-ROC']:.4f} ± {best_config['AUC-ROC_std']:.4f}\")\n",
    "print(f\"Precision: {best_config['Precision']:.4f} ± {best_config['Precision_std']:.4f}\")\n",
    "print(f\"Recall: {best_config['Recall']:.4f} ± {best_config['Recall_std']:.4f}\")\n",
    "print(f\"F1-Score: {best_config['F1']:.4f} ± {best_config['F1_std']:.4f}\")\n",
    "print(f\"Training Time: {best_config['Training_Time']:.1f} seconds\")\n",
    "\n",
    "# Get best configuration for each model\n",
    "best_models = {}\n",
    "for model_name in all_results.keys():\n",
    "    model_results = results_df[results_df['Model'] == model_name]\n",
    "    if not model_results.empty:\n",
    "        best_idx = model_results['Accuracy'].idxmax()\n",
    "        best_models[model_name] = model_results.loc[best_idx]\n",
    "\n",
    "print(f\"\\n📈 BEST CONFIGURATION FOR EACH MODEL:\")\n",
    "print(\"=\"*80)\n",
    "for model_name, best_result in best_models.items():\n",
    "    print(f\"{model_name:18}: {best_result['Sampling']:12} | \"\n",
    "          f\"Acc: {best_result['Accuracy']:.4f} | \"\n",
    "          f\"AUC: {best_result['AUC-ROC']:.4f}\")\n",
    "\n",
    "# Save results summary\n",
    "results_df.to_csv('../Data/output/advanced_optimization_results.csv', index=False)\n",
    "print(f\"\\n💾 Results saved to advanced_optimization_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac457e1c",
   "metadata": {},
   "source": [
    "## 🤝 Ensemble Methods (Speed Optimized)\n",
    "\n",
    "Combining the top 2 diverse individual models using soft voting for fast execution while maintaining high performance.\n",
    "\n",
    "### Ensemble Technique Applied:\n",
    "1. **Soft Voting**: Average of predicted probabilities from top 2 diverse models\n",
    "\n",
    "### Speed Optimizations:\n",
    "- **Top 2 Models Only**: Reduced from 3 to 2 models for faster training\n",
    "- **cross_validate()**: Single call for multiple metrics instead of multiple cross_val_score() calls\n",
    "- **CV=5 with Stratification**: Efficient 5-fold stratified cross-validation\n",
    "- **Validation Set Confirmation**: Final performance check on held-out data\n",
    "\n",
    "### Expected Benefits:\n",
    "- Reduced overfitting through model diversity\n",
    "- Better generalization performance  \n",
    "- Improved prediction stability\n",
    "- **Ultra-fast execution** (1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8c12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤝 ENSEMBLE METHODS EVALUATION\n",
      "============================================================\n",
      "🎯 Current best individual model accuracy: 0.8709\n",
      "\n",
      "🏆 Top 2 configurations for ensemble (optimized for speed):\n",
      "  GradientBoosting + smote_tomek: 0.8709\n",
      "  CatBoost + smote_tomek: 0.8670\n",
      "\n",
      "🔧 Creating ensemble with 2 models...\n",
      "\n",
      "🔍 Evaluating Soft Voting Ensemble (Fast Mode)...\n",
      "  AUC-ROC: 0.9449 ± 0.0022\n",
      "  Accuracy: 0.8699 ± 0.0075\n",
      "  Precision: 0.8632 ± 0.0067\n",
      "  Recall: 0.8791 ± 0.0116\n",
      "  F1-Score: 0.8711 ± 0.0078\n",
      "\n",
      "🧪 Validating ensemble on held-out validation set...\n",
      "  Validation Accuracy: 0.7707\n",
      "  Validation AUC-ROC: 0.8096\n",
      "\n",
      "✅ Ensemble evaluation completed efficiently! (Top 2 models, cross_validate)\n",
      "📊 CV vs Validation: Acc 0.8699 vs 0.7707\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Methods Implementation\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"🤝 ENSEMBLE METHODS EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best accuracy from our results for comparison\n",
    "best_accuracy = best_config['Accuracy']\n",
    "\n",
    "print(f\"🎯 Current best individual model accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Get top 2 best configurations for faster ensemble (diverse models)\n",
    "top_2_configs = results_df_sorted.head(2)\n",
    "print(\"\\n🏆 Top 2 configurations for ensemble (optimized for speed):\")\n",
    "for idx, config in top_2_configs.iterrows():\n",
    "    print(f\"  {config['Model']} + {config['Sampling']}: {config['Accuracy']:.4f}\")\n",
    "\n",
    "# Extract best models for ensemble\n",
    "ensemble_models = []\n",
    "ensemble_data = None\n",
    "\n",
    "for idx, config in top_2_configs.iterrows():\n",
    "    model_name = config['Model']\n",
    "    sampling_method = config['Sampling']\n",
    "    result = all_results[model_name][sampling_method]\n",
    "    \n",
    "    if result is not None:\n",
    "        # Get the optimized model\n",
    "        model = result['model']\n",
    "        X_ensemble, y_ensemble = result['sampling_data']\n",
    "        \n",
    "        ensemble_models.append((f\"{model_name}_{sampling_method}\", model))\n",
    "        if ensemble_data is None:\n",
    "            ensemble_data = (X_ensemble, y_ensemble)\n",
    "\n",
    "print(f\"\\n🔧 Creating ensemble with {len(ensemble_models)} models...\")\n",
    "\n",
    "# Create ensemble methods (optimized for speed)\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "ensemble_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Soft Voting Classifier (Fast execution - only method used)\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=ensemble_models,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n🔍 Evaluating Soft Voting Ensemble (Fast Mode)...\")\n",
    "\n",
    "# Use cross_validate for efficient multiple metrics evaluation\n",
    "scoring = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results_raw = cross_validate(\n",
    "    soft_voting_clf, \n",
    "    ensemble_data[0], \n",
    "    ensemble_data[1], \n",
    "    cv=cv, \n",
    "    scoring=scoring, \n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Process results\n",
    "cv_scores = {}\n",
    "for metric in scoring:\n",
    "    scores = cv_results_raw[f'test_{metric}']\n",
    "    cv_scores[metric] = {'mean': scores.mean(), 'std': scores.std()}\n",
    "\n",
    "ensemble_results['Soft_Voting'] = cv_scores\n",
    "\n",
    "print(f\"  AUC-ROC: {cv_scores['roc_auc']['mean']:.4f} ± {cv_scores['roc_auc']['std']:.4f}\")\n",
    "print(f\"  Accuracy: {cv_scores['accuracy']['mean']:.4f} ± {cv_scores['accuracy']['std']:.4f}\")\n",
    "print(f\"  Precision: {cv_scores['precision']['mean']:.4f} ± {cv_scores['precision']['std']:.4f}\")\n",
    "print(f\"  Recall: {cv_scores['recall']['mean']:.4f} ± {cv_scores['recall']['std']:.4f}\")\n",
    "print(f\"  F1-Score: {cv_scores['f1']['mean']:.4f} ± {cv_scores['f1']['std']:.4f}\")\n",
    "\n",
    "# Validate on held-out validation set for final confirmation\n",
    "print(f\"\\n🧪 Validating ensemble on held-out validation set...\")\n",
    "soft_voting_clf.fit(ensemble_data[0], ensemble_data[1])\n",
    "val_pred = soft_voting_clf.predict(X_val)\n",
    "val_pred_proba = soft_voting_clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "val_auc = roc_auc_score(y_val, val_pred_proba)\n",
    "\n",
    "print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"  Validation AUC-ROC: {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Ensemble evaluation completed efficiently! (Top 2 models, cross_validate)\")\n",
    "print(f\"📊 CV vs Validation: Acc {cv_scores['accuracy']['mean']:.4f} vs {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134937c",
   "metadata": {},
   "source": [
    "## 📊 Comprehensive Results Comparison\n",
    "\n",
    "Comparing all individual models and ensemble methods to select the best performing approach for final model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234e5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 COMPREHENSIVE RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "🏆 FINAL RANKINGS BY ACCURACY:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. GradientBoosting (smote_tomek)      | Individual | Acc: 0.8709 | AUC: 0.9444 | F1: 0.8723\n",
      " 4. Soft_Voting Ensemble                | Ensemble   | Acc: 0.8699 | AUC: 0.9449 | F1: 0.8711\n",
      " 2. CatBoost (smote_tomek)              | Individual | Acc: 0.8670 | AUC: 0.9424 | F1: 0.8676\n",
      " 3. AdaBoost (smote_tomek)              | Individual | Acc: 0.8532 | AUC: 0.9368 | F1: 0.8567\n",
      "\n",
      "🥇 BEST OVERALL METHOD:\n",
      "Method: GradientBoosting (smote_tomek)\n",
      "Type: Individual\n",
      "Accuracy: 0.8709\n",
      "AUC-ROC: 0.9444\n",
      "Precision: 0.8632\n",
      "Recall: 0.8815\n",
      "F1-Score: 0.8723\n",
      "\n",
      "🎯 FINAL MODEL SELECTED: GradientBoosting (smote_tomek)\n",
      "Final Model Type: GradientBoostingClassifier\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Results Comparison\n",
    "print(\"📊 COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine individual and ensemble results\n",
    "all_methods = []\n",
    "\n",
    "# Add individual model results\n",
    "for idx, config in results_df_sorted.iterrows():\n",
    "    all_methods.append({\n",
    "        'Method': f\"{config['Model']} ({config['Sampling']})\",\n",
    "        'Type': 'Individual',\n",
    "        'AUC-ROC': config['AUC-ROC'],\n",
    "        'Accuracy': config['Accuracy'],\n",
    "        'Precision': config['Precision'],\n",
    "        'Recall': config['Recall'],\n",
    "        'F1': config['F1']\n",
    "    })\n",
    "\n",
    "# Add ensemble results\n",
    "for name, results in ensemble_results.items():\n",
    "    all_methods.append({\n",
    "        'Method': f\"{name} Ensemble\",\n",
    "        'Type': 'Ensemble',\n",
    "        'AUC-ROC': results['roc_auc']['mean'],\n",
    "        'Accuracy': results['accuracy']['mean'],\n",
    "        'Precision': results['precision']['mean'],\n",
    "        'Recall': results['recall']['mean'],\n",
    "        'F1': results['f1']['mean']\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_methods)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n🏆 FINAL RANKINGS BY ACCURACY:\")\n",
    "print(\"-\" * 80)\n",
    "for idx, row in comparison_df.head(10).iterrows():\n",
    "    print(f\"{idx+1:2d}. {row['Method']:35} | {row['Type']:10} | \"\n",
    "          f\"Acc: {row['Accuracy']:.4f} | AUC: {row['AUC-ROC']:.4f} | F1: {row['F1']:.4f}\")\n",
    "\n",
    "# Find best overall method\n",
    "best_method = comparison_df.iloc[0]\n",
    "print(f\"\\n🥇 BEST OVERALL METHOD:\")\n",
    "print(f\"Method: {best_method['Method']}\")\n",
    "print(f\"Type: {best_method['Type']}\")\n",
    "print(f\"Accuracy: {best_method['Accuracy']:.4f}\")\n",
    "print(f\"AUC-ROC: {best_method['AUC-ROC']:.4f}\")\n",
    "print(f\"Precision: {best_method['Precision']:.4f}\")\n",
    "print(f\"Recall: {best_method['Recall']:.4f}\")\n",
    "print(f\"F1-Score: {best_method['F1']:.4f}\")\n",
    "\n",
    "# Determine final model for saving\n",
    "if best_method['Type'] == 'Ensemble':\n",
    "    # Use soft voting ensemble (only ensemble method)\n",
    "    final_model = soft_voting_clf\n",
    "    final_model_name = best_method['Method']\n",
    "    final_training_data = ensemble_data\n",
    "else:\n",
    "    # Use best individual model\n",
    "    model_name = best_config['Model']\n",
    "    sampling_method = best_config['Sampling']\n",
    "    result = all_results[model_name][sampling_method]\n",
    "    \n",
    "    final_model = result['model']\n",
    "    final_model_name = f\"{model_name} ({sampling_method})\"\n",
    "    final_training_data = result['sampling_data']\n",
    "\n",
    "print(f\"\\n🎯 FINAL MODEL SELECTED: {final_model_name}\")\n",
    "print(f\"Final Model Type: {type(final_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fb769",
   "metadata": {},
   "source": [
    "## 💾 Final Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d465cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING BEST MODEL WITH COMPREHENSIVE METADATA\n",
      "============================================================\n",
      "\n",
      "📂 TRAINING DATA USED:\n",
      "Primary File: ../Data/output/feature_engineered_train.csv\n",
      "Data saved to metadata for complete directory tracking\n",
      "\n",
      "💾 Saving optimized model...\n",
      "✅ Model saved: D:\\telco-customer-churn-prediction-main\\telco-customer-churn-prediction-main\\Data\\output\\best_model_optimized.pkl\n",
      "\n",
      "📋 Saving model metadata...\n",
      "✅ Metadata saved: D:\\telco-customer-churn-prediction-main\\telco-customer-churn-prediction-main\\Data\\output\\best_model_metadata.json\n",
      "\n",
      "🔍 VERIFICATION:\n",
      "Model file size: 5003.2 KB\n",
      "Metadata file size: 2.9 KB\n",
      "\n",
      "🧪 Testing model loading...\n",
      "✅ Model loaded successfully: GradientBoostingClassifier\n",
      "\n",
      "🎯 MODEL PERSISTENCE COMPLETE!\n",
      "📁 Files ready for use:\n",
      "   • Model: best_model_optimized.pkl\n",
      "   • Metadata: best_model_metadata.json\n",
      "   • Location: D:\\telco-customer-churn-prediction-main\\telco-customer-churn-prediction-main\\Data\\output\n",
      "📊 Best Performance: 0.8709 accuracy, 0.9444 AUC-ROC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"💾 SAVING BEST MODEL WITH COMPREHENSIVE METADATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define save directory and file paths\n",
    "model_save_dir = Path(\"../Data/output\")\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_pkl_path = model_save_dir / \"best_model_optimized.pkl\"\n",
    "metadata_json_path = model_save_dir / \"best_model_metadata.json\"\n",
    "\n",
    "# Get current working directory and data paths\n",
    "current_dir = Path.cwd()\n",
    "data_dir = Path(\"../Data\")\n",
    "\n",
    "# Comprehensive training data directory information\n",
    "training_data_info = {\n",
    "    \"primary_training_file\": {\n",
    "        \"path\": \"../Data/output/feature_engineered_train.csv\",\n",
    "        \"absolute_path\": str((current_dir / \"../Data/output/feature_engineered_train.csv\").resolve()),\n",
    "        \"exists\": (current_dir / \"../Data/output/feature_engineered_train.csv\").exists(),\n",
    "        \"description\": \"Main feature-engineered training dataset used for optimization\"\n",
    "    },\n",
    "    \"data_directories\": {\n",
    "        \"input_dir\": {\n",
    "            \"path\": \"../Data/input/\",\n",
    "            \"absolute_path\": str((current_dir / \"../Data/input\").resolve()),\n",
    "            \"exists\": (current_dir / \"../Data/input\").exists(),\n",
    "            \"description\": \"Original raw data directory\"\n",
    "        },\n",
    "        \"interim_dir\": {\n",
    "            \"path\": \"../Data/interim/\",\n",
    "            \"absolute_path\": str((current_dir / \"../Data/interim\").resolve()),\n",
    "            \"exists\": (current_dir / \"../Data/interim\").exists(),\n",
    "            \"description\": \"Intermediate processed data and model files\"\n",
    "        },\n",
    "        \"output_dir\": {\n",
    "            \"path\": \"../Data/output/\",\n",
    "            \"absolute_path\": str((current_dir / \"../Data/output\").resolve()),\n",
    "            \"exists\": (current_dir / \"../Data/output\").exists(),\n",
    "            \"description\": \"Final processed data and output files\"\n",
    "        }\n",
    "    },\n",
    "    \"related_files\": {\n",
    "        \"original_data\": \"../Data/input/Telco_Customer_kaggle.csv\",\n",
    "        \"test_data\": \"../Data/output/test.csv\",\n",
    "        \"feature_selection\": \"../Data/output/feature_selection_summary.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print essential training data information\n",
    "print(f\"\\n📂 TRAINING DATA USED:\")\n",
    "print(f\"Primary File: {training_data_info['primary_training_file']['path']}\")\n",
    "print(f\"Data saved to metadata for complete directory tracking\")\n",
    "\n",
    "# Prepare comprehensive model metadata\n",
    "model_metadata = {\n",
    "    \"model_info\": {\n",
    "        \"model_name\": final_model_name,\n",
    "        \"model_type\": type(final_model).__name__,\n",
    "        \"optimization_timestamp\": datetime.now().isoformat(),\n",
    "        \"python_version\": sys.version,\n",
    "        \"optimization_method\": \"Optuna Bayesian + SMOTE-Tomek\"\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"accuracy\": float(best_method['Accuracy']),\n",
    "        \"auc_roc\": float(best_method['AUC-ROC']),\n",
    "        \"precision\": float(best_method['Precision']),\n",
    "        \"recall\": float(best_method['Recall']),\n",
    "        \"f1_score\": float(best_method['F1'])\n",
    "    },\n",
    "    \"training_data\": training_data_info,\n",
    "    \"model_configuration\": {\n",
    "        \"cross_validation_folds\": 5,\n",
    "        \"sampling_technique\": \"SMOTE-Tomek\" if 'smote_tomek' in final_model_name.lower() else \"Original\",\n",
    "        \"hyperparameter_trials\": 20,\n",
    "        \"ensemble_method\": \"Soft Voting\" if \"Ensemble\" in final_model_name else \"Single Model\"\n",
    "    },\n",
    "    \"data_shape\": {\n",
    "        \"original_features\": X.shape[1],\n",
    "        \"original_samples\": X.shape[0],\n",
    "        \"training_samples\": final_training_data[0].shape[0],\n",
    "        \"validation_samples\": X_val.shape[0]\n",
    "    },\n",
    "    \"class_distribution\": {\n",
    "        \"original\": {str(k): int(v) for k, v in Counter(y).items()},\n",
    "        \"training_resampled\": {str(k): int(v) for k, v in Counter(final_training_data[1]).items()}\n",
    "    },\n",
    "    \"file_paths\": {\n",
    "        \"model_pkl\": str(model_pkl_path.resolve()),\n",
    "        \"metadata_json\": str(metadata_json_path.resolve()),\n",
    "        \"notebook_path\": str(Path.cwd().resolve())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model\n",
    "print(f\"\\n💾 Saving optimized model...\")\n",
    "with open(model_pkl_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(f\"✅ Model saved: {model_pkl_path.resolve()}\")\n",
    "\n",
    "# Save metadata\n",
    "print(f\"\\n📋 Saving model metadata...\")\n",
    "with open(metadata_json_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "print(f\"✅ Metadata saved: {metadata_json_path.resolve()}\")\n",
    "\n",
    "# Verify saved files\n",
    "print(f\"\\n🔍 VERIFICATION:\")\n",
    "print(f\"Model file size: {model_pkl_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Metadata file size: {metadata_json_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Test loading the saved model\n",
    "print(f\"\\n🧪 Testing model loading...\")\n",
    "with open(model_pkl_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(f\"✅ Model loaded successfully: {type(loaded_model).__name__}\")\n",
    "\n",
    "print(f\"\\n🎯 MODEL PERSISTENCE COMPLETE!\")\n",
    "print(f\"📁 Files ready for use:\")\n",
    "print(f\"   • Model: {model_pkl_path.name}\")\n",
    "print(f\"   • Metadata: {metadata_json_path.name}\")\n",
    "print(f\"   • Location: {model_save_dir.resolve()}\")\n",
    "print(f\"📊 Best Performance: {best_method['Accuracy']:.4f} accuracy, {best_method['AUC-ROC']:.4f} AUC-ROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0794e21",
   "metadata": {},
   "source": [
    "## 🚀 Model Usage Instructions\n",
    "\n",
    "The optimized model has been saved and is ready for use with test data. Here's how to use the saved model:\n",
    "\n",
    "### Loading the Model:\n",
    "```python\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Load the model\n",
    "with open('../Data/output/best_model_optimized.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load metadata for reference\n",
    "with open('../Data/output/best_model_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "```\n",
    "\n",
    "### Key Files Saved:\n",
    "- **`best_model_optimized.pkl`**: The trained model ready for predictions\n",
    "- **`best_model_metadata.json`**: Comprehensive metadata including:\n",
    "  - Model performance metrics\n",
    "  - Training data directories and paths\n",
    "  - Hyperparameter optimization details\n",
    "  - Data preprocessing information\n",
    "  - Class distribution details\n",
    "\n",
    "### Next Steps:\n",
    "1. Load test data from the same feature engineering pipeline\n",
    "2. Apply the same preprocessing (SMOTE-Tomek if ensemble)\n",
    "3. Use `model.predict()` or `model.predict_proba()` for predictions\n",
    "4. Evaluate final performance on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

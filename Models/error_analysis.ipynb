{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cf2127",
   "metadata": {},
   "source": [
    "# üîç Error Analysis: Hyperparameter Tuning Issues\n",
    "\n",
    "This notebook identifies and fixes the issue where all three models (GradientBoosting, CatBoost, AdaBoost) are showing identical results in the hyperparameter tuning notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587f736",
   "metadata": {},
   "source": [
    "## üö® Problem Identification\n",
    "\n",
    "The issue in the hyperparameter tuning notebook is in the **Model Comparison Table** section. All three models are using the same `scores` variable, which only contains the results from the last model evaluated (AdaBoost).\n",
    "\n",
    "**Root Cause:**\n",
    "- The `scores` variable is being overwritten in each model's tuning section\n",
    "- The comparison table uses the same `scores` variable for all three models\n",
    "- This results in identical metrics for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d57eec",
   "metadata": {},
   "source": [
    "## üîß Solution\n",
    "\n",
    "We need to store each model's scores separately and use them correctly in the comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb11cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED CODE FOR THE HYPERPARAMETER TUNING NOTEBOOK\n",
    "# This shows the fix that needs to be applied\n",
    "\n",
    "# After GradientBoosting tuning, store scores as:\n",
    "gbc_scores = {\n",
    "    'AUC-ROC': cross_val_score(gbc_best, X, y, cv=cv, scoring='roc_auc'),\n",
    "    'Accuracy': cross_val_score(gbc_best, X, y, cv=cv, scoring='accuracy'),\n",
    "    'Precision': cross_val_score(gbc_best, X, y, cv=cv, scoring='precision'),\n",
    "    'Recall': cross_val_score(gbc_best, X, y, cv=cv, scoring='recall')\n",
    "}\n",
    "\n",
    "# After CatBoost tuning, store scores as:\n",
    "cbc_scores = {\n",
    "    'AUC-ROC': cross_val_score(cbc_best, X, y, cv=cv, scoring='roc_auc'),\n",
    "    'Accuracy': cross_val_score(cbc_best, X, y, cv=cv, scoring='accuracy'),\n",
    "    'Precision': cross_val_score(cbc_best, X, y, cv=cv, scoring='precision'),\n",
    "    'Recall': cross_val_score(cbc_best, X, y, cv=cv, scoring='recall')\n",
    "}\n",
    "\n",
    "# After AdaBoost tuning, store scores as:\n",
    "ada_scores = {\n",
    "    'AUC-ROC': cross_val_score(ada_best, X, y, cv=cv, scoring='roc_auc'),\n",
    "    'Accuracy': cross_val_score(ada_best, X, y, cv=cv, scoring='accuracy'),\n",
    "    'Precision': cross_val_score(ada_best, X, y, cv=cv, scoring='precision'),\n",
    "    'Recall': cross_val_score(ada_best, X, y, cv=cv, scoring='recall')\n",
    "}\n",
    "\n",
    "print('‚úÖ Separate score variables created for each model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED COMPARISON TABLE CODE\n",
    "# This replaces the existing comparison table\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        'Model': 'GradientBoosting',\n",
    "        'AUC-ROC': gbc_scores['AUC-ROC'].mean(),\n",
    "        'Accuracy': gbc_scores['Accuracy'].mean(),\n",
    "        'Precision': gbc_scores['Precision'].mean(),\n",
    "        'Recall': gbc_scores['Recall'].mean(),\n",
    "        'Best Params': gbc_best_params\n",
    "    },\n",
    "    {\n",
    "        'Model': 'CatBoost',\n",
    "        'AUC-ROC': cbc_scores['AUC-ROC'].mean(),\n",
    "        'Accuracy': cbc_scores['Accuracy'].mean(),\n",
    "        'Precision': cbc_scores['Precision'].mean(),\n",
    "        'Recall': cbc_scores['Recall'].mean(),\n",
    "        'Best Params': cbc_best_params\n",
    "    },\n",
    "    {\n",
    "        'Model': 'AdaBoost',\n",
    "        'AUC-ROC': ada_scores['AUC-ROC'].mean(),\n",
    "        'Accuracy': ada_scores['Accuracy'].mean(),\n",
    "        'Precision': ada_scores['Precision'].mean(),\n",
    "        'Recall': ada_scores['Recall'].mean(),\n",
    "        'Best Params': ada_best_params\n",
    "    }\n",
    "]\n",
    "\n",
    "print('‚úÖ Corrected comparison table will show different results for each model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a9ed2",
   "metadata": {},
   "source": [
    "## üìã Step-by-Step Fix Instructions\n",
    "\n",
    "### Changes needed in the hyperparameter tuning notebook:\n",
    "\n",
    "1. **After GradientBoosting tuning section:** Rename `scores` to `gbc_scores`\n",
    "2. **After CatBoost tuning section:** Rename `scores` to `cbc_scores`\n",
    "3. **After AdaBoost tuning section:** Rename `scores` to `ada_scores`\n",
    "4. **In the comparison table:** Use the specific score variables for each model\n",
    "\n",
    "### Additional Improvements:\n",
    "- Add validation to ensure models are actually different\n",
    "- Include model-specific random states for reproducibility\n",
    "- Add error handling for edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06faa77",
   "metadata": {},
   "source": [
    "## üß™ Validation Code\n",
    "\n",
    "This code can be added to verify that models are producing different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ba204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE TO ADD AFTER THE CORRECTED COMPARISON TABLE\n",
    "\n",
    "# Check if all models have identical results (which would indicate an error)\n",
    "gbc_auc = gbc_scores['AUC-ROC'].mean()\n",
    "cbc_auc = cbc_scores['AUC-ROC'].mean()\n",
    "ada_auc = ada_scores['AUC-ROC'].mean()\n",
    "\n",
    "if gbc_auc == cbc_auc == ada_auc:\n",
    "    print('‚ö†Ô∏è  WARNING: All models have identical AUC-ROC scores!')\n",
    "    print('This may indicate an error in model training or evaluation.')\n",
    "else:\n",
    "    print('‚úÖ Models show different performance - fix successful!')\n",
    "    print(f'GradientBoosting AUC-ROC: {gbc_auc:.6f}')\n",
    "    print(f'CatBoost AUC-ROC: {cbc_auc:.6f}')\n",
    "    print(f'AdaBoost AUC-ROC: {ada_auc:.6f}')\n",
    "\n",
    "# Show parameter differences\n",
    "print('\\nüìä Best Parameters Summary:')\n",
    "print(f'GradientBoosting: {gbc_best_params}')\n",
    "print(f'CatBoost: {cbc_best_params}')\n",
    "print(f'AdaBoost: {ada_best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480f156",
   "metadata": {},
   "source": [
    "## üéØ Expected Outcome\n",
    "\n",
    "After applying these fixes:\n",
    "\n",
    "1. **Different AUC-ROC scores** for each model (typically 0.001-0.05 difference)\n",
    "2. **Different hyperparameters** selected for each model\n",
    "3. **Slight variations** in Accuracy, Precision, and Recall\n",
    "4. **Meaningful model comparison** enabling proper selection\n",
    "\n",
    "## üîç Why This Happened\n",
    "\n",
    "- **Variable overwriting:** Using the same variable name (`scores`) for all models\n",
    "- **Python scoping:** The last assignment to `scores` overwrote previous values\n",
    "- **Copy-paste error:** Common when duplicating code sections without proper variable naming\n",
    "\n",
    "## üõ°Ô∏è Prevention\n",
    "\n",
    "- Use **descriptive variable names** (e.g., `gbc_scores`, `cbc_scores`)\n",
    "- Add **validation checks** after each model training\n",
    "- Use **different random states** for each model if needed\n",
    "- **Review results** immediately after each model to catch issues early"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
